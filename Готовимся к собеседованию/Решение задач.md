# Архитектурная задача

"Дано:
REST API VK и Yandex, предоставляющие данные:
1) рекламная компания (РК)
2) объявления
3) статистика просмотра объявлений

Имеется следующая инфраструктура: Airflow, Impala (с поддержкой Iceberg), Kafka, Kubernetes, S3 (MinIO), Spark, Python, Clickhouse, Superset

Требуется:
1) получать данные из источников каждый день
2) сохранять в сыром виде
3) обогащать, консолидировать, в нормализованном виде сохранять в DWH
4) агрегировать и формировать отчет по эффективности РК
5) отправлять email с этим отчетом рассылкой в адрес нужных сотрудников"

**Решение:**

триггерим API (Python + Airflow)
сохраняем в lake S3 (MinIO)
читаем из lake (Spark)
преобразуем 
отправляем в DWH (Clickhouse)
читаем или транслируем в BI инструмент (Superset)

Kafka - журнал процессной системы

### SQL задачи
Оконные функции и как пользоваться окном
https://postgrespro.ru/docs/postgrespro/current/tutorial-window
https://postgrespro.ru/docs/postgrespro/current/functions-window


# SQL

## Баланса счета во все моменты времени. Нарастающий остаток. Оконная функция для проставления 

OperationID - numeric(15,0) - Уникальный идентификатор операции
CharType - int - Тип операции
OperDate - datetime -  Дата операции
Qty - numeric(28,10) -  Сумма операции
Indatetime - datetime - Фактическая дата совершения операции
Rest - numeric(28,10) -  Остаток

```sql

-- Создание структуры
CREATE TABLE #OperPart
(
  OperationID  numeric(15,0)
 ,CharType     int
 ,OperDate     datetime
 ,Qty          numeric(28,10)
 ,Indatetime   datetime
 ,Rest         numeric(28,10)
)
 
-- Наполнение данными
INSERT INTO #OperPart VALUES( 33581974000,  1,  '20210820 0:00:00.000', 1000011.0000000000, '20210820 13:10:25.700',    0 )
INSERT INTO #OperPart VALUES( 33582024800,  -1, '20210820 0:00:00.000', 1000011.1100000000, '20210820 13:51:08.903',    0 )
INSERT INTO #OperPart VALUES( 33582208700,  1,  '20210820 0:00:00.000', 825000.1500000000,  '20210820 15:33:38.776',    0 )
INSERT INTO #OperPart VALUES( 33582407110,  -1, '20210820 0:00:00.000', 825000.0000000000,  '20210820 16:29:39.980',    0 )
INSERT INTO #OperPart VALUES( 33582408610,  1,  '20210820 0:00:00.000', 1000011.0000000000, '20210820 16:34:53.183',    0 )
INSERT INTO #OperPart VALUES( 33582419620,  -1, '20210820 0:00:00.000', 1000011.0000000000, '20210820 17:09:24.250',    0 )
INSERT INTO #OperPart VALUES( 33582519290,  1,  '20210820 0:00:00.000', 15000.0000000000,   '20210820 19:57:20.546',    0 )
```
В таблице содержатся записи об операциях по одному счету. Если CharType = -1 , то операция пополнения счета, если 1 – операция списания со счета

Остаток на начало дня 20.08.2021 по счету 1000868.31

Необходимо для каждой операции в таблице #OperPart заполнить поле Rest 
актуальным остатком (под актуальным остатком понимается остаток, который образовался после совершения данной операции)

**Решение:**

```sql
-- Это запрос с использованием оконной функции дает верный ответ
SELECT  
    OperationID,  
    CharType,  
    OperDate,  
    Qty,  
    Indatetime,  
    1000868.31 + SUM(CASE WHEN CharType = -1 THEN Qty ELSE -Qty END)  
        OVER (ORDER BY Indatetime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS Rest  
FROM #OperPart  
ORDER BY Indatetime;

-- Если фрейм не указан явно, SQL использует значение по умолчанию:
-- - Без ORDER BY: Все строки в партиции (или таблице).
-- - С ORDER BY: RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW (все строки от начала до текущего значения столбца сортировки).
-- Но можно и так, потому что OVER имеет дефолтное значение ()

SELECT  
    OperationID,  
    CharType,  
    OperDate,  
    Qty,  
    Indatetime,  
    1000868.31 + SUM(CASE WHEN CharType = -1 THEN Qty ELSE -Qty END)  
        OVER (ORDER BY Indatetime) AS Rest  
FROM #OperPart  
ORDER BY Indatetime;
```

## Количество строк при разных join

Даны две таблицы T1 (col1, col2) и T2 (col1, col3)

В таблице T1 три записи
В таблице T2 четыре записи

Оцените какое количество строк вернут операторы 
```sql
T1 join T2 on T1.col1 = T2.col1 
min:0
max:12

T1 left join T2 on T1.col1 = T2.col1 
min:3
max:12

T1 full join T2 on T1.col1 = T2.col1 
min:7
max:12

T1 cross join T2
min:12
max:12
```
## Дана таблица T(value), выведите все повторяющиеся записи

T - название таблицы
value - название атрибута

```sql
CREATE temporary TABLE T1 (col1 Int);  
  
INSERT INTO T1 VALUES  
(1),  
(1),  
(1),  
(3),  
(4),  
(5),  
(1),  
(3);
    
select col1, count(*)  
from T1  
group by col1  
having count(*)>1
```

## Вывести сотрудника и департамент, со второй максимальной зарплатой в каждом департаменте за апрель 2023

Salary
VALUE_DAY     EMPL_FIO                      EMPL_DEP         AMOUNT
30.04.2023    Иванов Иван Иванович          Департамент 1    50000 1
30.04.2023    Сидоров Валерий Валериевич    Департамент 1    75000 2
31.05.2023    Иванов Иван Иванович          Департамент 2    84000
30.04.2023    Сидоров Иван Иванович         Департамент 2    75000
30.04.2023    Иванов Петр Петрович          Департамент 2    80000
31.05.2023    Сидоров Валерий Валериевич    Департамент 1    88000
30.04.2023    Петров Валерий Валериевич     Департамент 2    89000
31.05.2023    Петров Петр Петрович          Департамент 1    89000
31.05.2023    Сидоров Иван Иванович         Департамент 2    89000
31.05.2023    Иванов Петр Петрович          Департамент 2    90000
31.05.2023    Петров Валерий Валериевич     Департамент 1    94000
30.04.2023    Петров Петр Петрович          Департамент 1    100000 

```SQL
with main_query as (  
    SELECT  
        EMPL_FIO,  
        EMPL_DEP,  
        AMOUNT,  
        RANK() OVER (PARTITION BY EMPL_DEP ORDER BY AMOUNT DESC) AS rnk  
    FROM Salary  
    WHERE VALUE_DAY BETWEEN '2023-04-01' AND '2023-04-30'  
)  
SELECT   
EMPL_FIO,  
    EMPL_DEP,  
    AMOUNT  
FROM main_query  
WHERE rnk = 2;
```

#### Выведите поквартальную статистику продаж в разрезе регионов:

Данные в таблицу вносятся вручную, поэтому возможны дефекты написания регионов.
Учтите это при выполнении задачи - регионы нужно привести к единому виду.
Структура таблицы Sales (id, value_day, region, total_price)
    id - номер чека
    value_day - дата продажи
    region - регион
    total_price - сумма чека

Пример данных
(1, '01.01.2024', 'Москва', 100)
(2, '01.02.2024', 'г. Москва', 500)
(3, '01.04.2024', 'москва', 50)
(4, '07.05.2024', 'Московская обл', 67)
(5, '11.01.2024', 'Московская область', 20)
(6, '03.04.2023', 'Московская обл.', 30)
(7, '01.06.2022', 'Санкт Петербург', 11)
(8, '03.04.2024', 'Г. Санкт-Петербург', 10)
(9, '12.07.2023', 'Тулькая обл', 30)
(10, '01.08.2023', 'Тверская обл.', 50)

Выведите поквартальную статистику продаж в разрезе регионов:
Объем продаж (руб)
Кол-во продаж (шт)
Средний чек (руб)
Максимальный чек (руб)
Медианный чек (руб)
Среднее кол-во продаж в день (шт)
Максимальный дневной объем продаж (руб)

```sql
with -- Заменяем название регионов  
    name_region as (select id,  
                            value_day,  
                            case  
                                when lower(region) like '%москва%' then 'Москва'  
                                when lower(region) like '%московская%' then 'Московская область'  
                                when lower(region) like '%петербург%' then 'Санкт-Петербург'  
                                when lower(region) like '%тулькая%' then 'Тулькая область'  
                                when lower(region) like '%тверская%' then 'Тверская область'  
                                end as region_name,  
                            total_price  
                     from sales_region),  
-- Агрегация значений для расчета по дням  
     name_region_day as (select count(t.id)        as sale_day,  
                                t.value_day,  
                                t.region_name,  
                                sum(t.total_price) as total_price_day  
                         from name_region as t  
                         group by t.region_name, t.value_day)  
-- Финальный запрос  
select extract(quarter from t1.value_day)           as quarter,  
       t1.region_name                               as region_name,  
       count(*)                                  as "Кол-во продаж",  
       sum(t1.total_price)                          as "Объем продаж",  
       avg(t1.total_price)                          as "Средний чек",  
       max(t1.total_price)                          as "Максимальный чек",  
       percentile_cont(0.5) WITHIN GROUP (ORDER BY t1.total_price) / 2 as "Медианный чек",  
       avg(nrd.sale_day)                         as "Среднее кол-во продаж в день",  
       max(nrd.total_price_day)                  as "Максимальный дневной объем продаж"  
from name_region as t1  
         left join name_region_day as nrd  
                   on t1.region_name = nrd.region_name and t1.value_day = nrd.value_day  
group by extract(quarter from t1.value_day), t1.region_name  
order by extract(quarter from t1.value_day), t1.region_name
```


---

## Задача Конкуренты

Есть таблица `competitors_prices` со следующими данными:

- sku_id_competitor — идентификатор SKU конкурента  
- competitor_id — идентификатор конкурента  
- promo_price_competitor — промо-цена конкурента  
- regular_price_competitor — регулярная цена конкурента  
- parsing_date — дата парсинга  

Нужно получить витрину, где для каждой комбинации `(sku_id_competitor, competitor_id, parsing_date)` остаётся **только одно значение цены**:
- если есть `promo_price_competitor` → берём её,  
- если промо отсутствует (`NULL`) → берём `regular_price_competitor`.

---

### DDL и наполнение таблицы

```sql
CREATE TABLE IF NOT EXISTS public.competitors_prices (
    sku_id_competitor     INT,
    competitor_id         INT,
    promo_price_competitor DECIMAL,
    regular_price_competitor DECIMAL,
    parsing_date          DATE
);

INSERT INTO competitors_prices VALUES
(15, 200, 147.0, 174.0, DATE '2025-08-30'),
(16, 200, NULL, 222.0, DATE '2025-08-27'),
(15, 200, NULL , 174.0, DATE '2025-08-30');
```

<details>
<summary>Показать результат</summary>

<table>
  <tr>
    <th>sku_id_competitor</th>
    <th>competitor_id</th>
    <th>price</th>
    <th>parsing_date</th>
  </tr>
  <tr>
    <td>15</td>
    <td>200</td>
    <td>147</td>
    <td>2025-08-30</td>
  </tr>
  <tr>
    <td>16</td>
    <td>200</td>
    <td>222</td>
    <td>2025-08-27</td>
  </tr>
</table>

</details>

<details>
<summary>Показать решение</summary>

```sql
WITH ranked AS (
    SELECT
        sku_id_competitor,
        competitor_id,
        promo_price_competitor,
        regular_price_competitor,
        parsing_date,
        COALESCE(promo_price_competitor, regular_price_competitor) AS price,
        ROW_NUMBER() OVER (
            PARTITION BY sku_id_competitor, competitor_id, parsing_date
            ORDER BY CASE WHEN promo_price_competitor IS NOT NULL THEN 1 ELSE 0 END DESC
        ) AS rn
    FROM public.competitors_prices
)
SELECT
    sku_id_competitor,
    competitor_id,
    price,
    parsing_date
FROM ranked
WHERE rn = 1;
```
</details>

***

## Задача: Запрос с использованием SELECT

Есть таблица `employees` с полями:  
- id  
- name  
- salary  
- department_id  

Нужно написать SQL-запрос, который возвращает имена сотрудников и
их зарплаты для всех сотрудников с зарплатой выше средней по всей таблице.
<details>
<summary>Показать решение</summary>

```sql
SELECT 
    name, 
    salary
FROM employees
WHERE salary > (SELECT AVG(salary) FROM employees);
```
</details>

---
## Задача: Оконные функции и подзапросы

В таблице `employees` напишите запрос, который возвращает:  
- имя сотрудника  
- зарплату  
- ранг по зарплате внутри своего `department_id` (нумерация по убыванию зарплаты).

<details>
<summary>Показать решение</summary>

```sql
SELECT 
    name,
    salary,
    department_id,
    RANK() OVER (
        PARTITION BY department_id 
        ORDER BY salary DESC
    ) AS salary_rank
FROM employees;
```  
</details>

---

## Задача Последняя загрузка по таблице за день

Есть таблица public.load_status со следующими данными:
	•	dt — дата/время загрузки
	•	table_name — имя таблицы/вьюхи
	•	status — статус загрузки

Нужно построить выборку/витрину,
где для каждой таблицы в рамках каждого календарного дня
остаётся только одна строка — с максимальной dt
(если в день было несколько загрузок). Отфильтровать последние 14 дней.

```sql
DROP TABLE IF EXISTS public.load_status;

CREATE TABLE public.load_status (
    id          bigserial PRIMARY KEY,          -- уникальный ключ
    dt          timestamp with time zone NOT NULL,  -- время загрузки
    table_name  text NOT NULL,                  -- имя таблицы/вьюхи
    status      text NOT NULL                   -- статус загрузки
);

INSERT INTO public.load_status (dt, table_name, status) VALUES
-- Таблица alpha — два прогона в один день
('2025-09-15 09:15:00+03', 'alpha.orders_daily', 'OK'),
('2025-09-15 11:45:00+03', 'alpha.orders_daily', 'OK'),

-- Таблица beta — три прогона в один день
('2025-09-15 07:20:00+03', 'beta.customer_snapshot', 'OK'),
('2025-09-15 09:55:00+03', 'beta.customer_snapshot', 'OK'),
('2025-09-15 14:10:00+03', 'beta.customer_snapshot', 'OK'),

-- Таблица gamma — один прогон сегодня, два вчера
('2025-09-15 08:30:00+03', 'gamma.product_catalog', 'OK'),
('2025-09-14 10:05:00+03', 'gamma.product_catalog', 'OK'),
('2025-09-14 16:25:00+03', 'gamma.product_catalog', 'OK'),

-- Таблица delta — разные даты
('2025-09-13 06:40:00+03', 'delta.pricing_rules', 'OK'),
('2025-09-14 07:55:00+03', 'delta.pricing_rules', 'OK'),
('2025-09-15 12:20:00+03', 'delta.pricing_rules', 'OK'),

-- Таблица epsilon — только один прогон
('2025-09-15 10:10:00+03', 'epsilon.stock_levels', 'OK');
```

<details>
<summary>Показать результат</summary>
<table>
  <tr>
    <th>dt</th>
    <th>table_name</th>
    <th>status</th>
    <th>day</th>
  </tr>
  <tr>
    <td>2025-09-15 14:10:00+03</td>
    <td>beta.customer_snapshot</td>
    <td>OK</td>
    <td>2025-09-15</td>
  </tr>
  <tr>
    <td>2025-09-15 12:20:00+03</td>
    <td>delta.pricing_rules</td>
    <td>OK</td>
    <td>2025-09-15</td>
  </tr>
  <tr>
    <td>2025-09-15 11:45:00+03</td>
    <td>alpha.orders_daily</td>
    <td>OK</td>
    <td>2025-09-15</td>
  </tr>
  <tr>
    <td>2025-09-15 10:10:00+03</td>
    <td>epsilon.stock_levels</td>
    <td>OK</td>
    <td>2025-09-15</td>
  </tr>
  <tr>
    <td>2025-09-15 08:30:00+03</td>
    <td>gamma.product_catalog</td>
    <td>OK</td>
    <td>2025-09-15</td>
  </tr>
  <tr>
    <td>2025-09-14 16:25:00+03</td>
    <td>gamma.product_catalog</td>
    <td>OK</td>
    <td>2025-09-14</td>
  </tr>
  <tr>
    <td>2025-09-14 07:55:00+03</td>
    <td>delta.pricing_rules</td>
    <td>OK</td>
    <td>2025-09-14</td>
  </tr>
  <tr>
    <td>2025-09-13 06:40:00+03</td>
    <td>delta.pricing_rules</td>
    <td>OK</td>
    <td>2025-09-13</td>
  </tr>
</table>
</details>

<details>
<summary>Показать решение</summary>

```sql
WITH ranked AS (
    SELECT
        ls.dt,
        ls.table_name,
        ls.status,
        ls.dt::date AS day,
        ROW_NUMBER() OVER (
            PARTITION BY ls.table_name, ls.dt::date
            ORDER BY ls.dt DESC
        ) AS rn
    FROM public.load_status AS ls
    WHERE ls.dt >= current_date - INTERVAL '14 days'
)
SELECT
    dt,
    table_name,
    status,
    day
FROM ranked
WHERE rn = 1
ORDER BY dt DESC;
```
</details>

---

## Задача: Создание функции

Создайте функцию `get_department_salary_summary(dept_id INT)`, которая принимает ID отдела и возвращает таблицу с:  
- общим количеством сотрудников  
- средней зарплатой в этом отделе.

<details>
<summary>Показать решение</summary>

```sql
CREATE OR REPLACE FUNCTION get_department_salary_summary(dept_id INT)
RETURNS TABLE (
    employees_count INT,
    avg_salary NUMERIC
)
AS $$
BEGIN
    RETURN QUERY
    SELECT 
        COUNT(*) AS employees_count,
        AVG(salary) AS avg_salary
    FROM employees
    WHERE department_id = dept_id;
END;
$$ LANGUAGE plpgsql;
```  
</details>

# Python

В этом разделе собраны практические задачи на Python, которые проверяют умение работать с данными и оптимизацией.  

---

## Задача: Выгрузка данных из PostgreSQL в CSV файл

Необходимо написать код на Python, который будет выгружать данные из таблицы PostgreSQL `big_table` и записывать их в CSV файл.  
Особенность: таблица содержит **40 миллионов строк**, поэтому данные нужно выгружать чанками, чтобы избежать переполнения памяти.

```python
import psycopg2
import csv

dsn = "host=... dbname=... user=... password=... port=5432"
outfile = "big_table.csv"
chunk_size = 100_000

with psycopg2.connect(dsn) as conn:
    # серверный курсор — результат не буферизуется целиком
    cur = conn.cursor(name="ss_cur")
    cur.itersize = chunk_size
    cur.execute("SELECT * FROM public.big_table ORDER BY id")  # важно иметь порядок/индекс

    with open(outfile, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        # заголовки
        colnames = [desc[0] for desc in cur.description]
        writer.writerow(colnames)

        rows_written = 0
        while True:
            rows = cur.fetchmany(chunk_size)
            if not rows:
                break
            writer.writerows(rows)
            rows_written += len(rows)
            # print(f"written: {rows_written}")
```

---

## Задача: Перенос данных из PostgreSQL в ClickHouse

Напишите код на Python, который переносит данные из таблицы PostgreSQL `clients` в таблицу ClickHouse `clients`.  
Данные выгружаются чанками, чтобы избежать переполнения памяти.

```python
import psycopg2
import clickhouse_connect

PG_DSN = "host=... dbname=... user=... password=... port=5432"
CH_HOST = "http://localhost:8123"   # или http://ch-host:8123
CH_USER = "default"
CH_PASS = ""
CH_DB   = "default"

CHUNK = 100_000

# 1) Читаем из PostgreSQL чанками
with psycopg2.connect(PG_DSN) as pg_conn:
    pg_cur = pg_conn.cursor(name="ss_clients")  # server-side курсор
    pg_cur.itersize = CHUNK
    pg_cur.execute("SELECT id, name, email, created_at FROM public.clients ORDER BY id")

    # 2) Подключаемся к ClickHouse
    ch = clickhouse_connect.get_client(host=CH_HOST, username=CH_USER, password=CH_PASS, database=CH_DB)
    # Опционально: создаём таблицу при отсутствии
    ch.query("""
        CREATE TABLE IF NOT EXISTS clients (
            id UInt64,
            name String,
            email String,
            created_at DateTime
        ) ENGINE = MergeTree
        ORDER BY id
    """)

    # 3) Гоним чанками
    total = 0
    columns = ["id", "name", "email", "created_at"]

    while True:
        rows = pg_cur.fetchmany(CHUNK)
        if not rows:
            break

        # Вставка пачкой
        ch.insert("clients", rows, column_names=columns)
        total += len(rows)
        # print(f"Inserted: {total}")
```



