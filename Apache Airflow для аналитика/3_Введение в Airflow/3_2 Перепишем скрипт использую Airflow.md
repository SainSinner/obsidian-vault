следующая [[2_2 Directed Acyclic Graph (DAG)]]


Прежде чем я представлю вам теорию о функционировании Airflow, его компонентах и наилучших практиках, давайте пересмотрим наш предыдущий код, применяя возможности Airflow. После этого мы более подробно рассмотрим каждый этап по отдельности.

```python
import pandas as pd
import sqlite3

CON = sqlite3.connect('example.db')

# Повторим предыдущий код с небольшим измененеием, теперь файлы 
# записываются на диск, а не передаются в потоке исполнения
# это особенность рабоыт Airflow которую мы обсудим позднее
# **context это необязательный аргумент, но мы будем его использовать далее
# это мощный инструмент который позволит нам писать идемпотентные скрипты

def extract_data(url, tmp_file, **context):
    pd.read_csv(url).to_csv(tmp_file) # Изменение to_csv, запишем данные в файл


def transform_data(group, agreg, tmp_file, tmp_agg_file, **context):
    data = pd.read_csv(tmp_file) # Изменение read_csv
    data.groupby(group).agg(agreg).reset_index().to_csv(tmp_agg_file) # Изменение to_csv, запишем данные в файл
 

def load_data(tmp_file, table_name, conn=CON, **context):
    data = pd.read_csv(tmp_file)# Изменение read_csv, прочитаем данные из файла
    data["insert_time"] = pd.to_datetime("now")
    data.to_sql(table_name, conn, if_exists='replace', index=False)
```

У вас может возникнуть вопрос почему мы записываем и читаем данные из файла, забегая вперед скажу что Airflow не позволяет вам обмениваться данными между различными функциями через оперативную память. Это связано с архитектурой системы, подробнее мы поговорим про это далее. Пока что примите это как данность.

Добавим теперь код который использоваться для запуска нашего кода Airflow:

```python
# Необходимые импорты
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.python_operator import PythonOperator

# Создаем DAG(контейнер) в который поместим наши задачи
# Для DAG-а характерны нужно задать следующие обязательные атрибуты
# - Уникальное имя
# - Интервал запусков
# - Начальная точка запуска

dag = DAG(dag_id='dag', # Имя нашего дага, уникальное
         default_args={'owner': 'airflow'}, # Список необязательных аргументов
         schedule_interval='@daily', # Интервал запусков, в данном случае 1 раз в день 24:00
         start_date=days_ago(1) # Начальная точка запуска, это с какого моменты мы бы хотели чтобы скрипт начал исполняться (далее разберем это подробнее)
    )

 
# Создадим задачу которая будет запускать питон функция
# Все именно так, создаем код для запуска другого кода
extract_data = PythonOperator(
    task_id='extract_data', # Имя задачи внутри Dag
    python_callable=extract_data, # Запускаемая Python функция, описана выше
        
    # Чтобы передать аргументы в нашу функцию 
    # их следует передавать через следующий код
    op_kwargs={
        'url': 'https://raw.githubusercontent.com/dm-novikov/stepik_airflow_course/main/data/data.csv',
        'tmp_file': '/tmp/file.csv'}
    )

transform_data = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    op_kwargs={
        'tmp_file': '/tmp/file.csv',
        'tmp_agg_file': '/tmp/file_agg.csv',
        'group': ['A', 'B', 'C'],
        'agreg': {"D": sum}},
    dag=dag
    )

load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    op_kwargs={
        'tmp_file': '/tmp/file_agg.csv',
        'table_name': 'table'},
    dag=dag
    )

# Создадим порядок выполнения задач
# В данном случае 2 задачи буудт последователньы и ещё 2 парараллельны
extract_data >> transform_data >> load_data 
```

Этот код выполняет то же действие, что и предыдущий, за исключением небольших изменений. Вы, возможно, обратили внимание, что появилась новая структура `DAG`. Мы уже обсуждали её ранее, но не в контексте Airflow. Кроме того, некоторые функции исчезли, а другие были видоизменены. Разберем данный код в следующих шагах.  
 

**Результат работы DAG**

Когда вы запустите данный код то на главной странице увидите следующее:

![](https://ucarecdn.com/41b0be41-79cd-43da-849e-0e7bcde8c8b9/)

Чтобы запустить данный DAG необходимо сместить ползунок вправо:

![](https://ucarecdn.com/4252c9a4-b733-42f6-994b-f7aaf2a50fab/)

После чего автоматически запустится скрипт который будет выполнять наш пайплан, чтобы посмотреть что происходит внутри, вы можете кликнуть на ссылку `dag` после чего вы должны увидеть:

![](https://ucarecdn.com/6c2789ca-879e-4504-99bd-a5dc2c85dcdb/)

Это наш настроенный пайплайн, мы запустили его 1 раз, каждый зеленый квадратик это состояние `success` то есть успешное выполнение. Про статусы выполнения задач мы поговорим позднее.

Чтобы посмотреть логи отработки каждой конкретной задачи, нужно кликнуть на квадратик, после чего выбрать ссылку `Logs`

![](https://ucarecdn.com/735744df-ecc2-441f-8b6e-ec341f954ea6/)

После перехода вы увидите следующее:

![](https://ucarecdn.com/b8dfaa42-539f-42ec-affd-7b79ae0b18b5/)

В случае если при исполнении ваш квадратик стал красным, то есть выполнение задачи завершилось ошибкой, то в логах будет отображена ошибка. Небольшой лайфхак, если вы используете `PythonOperator` то каждая команда

```scss
print(...)
```

Будет записана в данный лог файл, это позволит вам отладить работу вашего кода.

**Пример в облаке**

Ссылка на DAG: [http://158.160.116.58:8001/tree?dag_id=0_Examples_3.2.1](http://158.160.116.58:8001/tree?dag_id=0_Examples_3.2.1)

- example.db: это база данных используемая в коде
- table.csv: выгрузка из базы данных в csv

![](https://ucarecdn.com/a8e1a40b-8257-4133-9f03-46615129361d/)

**Дополнительные материалы**

- [Замечательная вводная статья по Airflow](https://habr.com/ru/post/512386/)