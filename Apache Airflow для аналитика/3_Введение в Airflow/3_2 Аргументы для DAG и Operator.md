предыдущая [[3_2 Operator и Task]]
следующая [[3_2 Зависимости и документация]]

На предыдущих этапах мы ознакомились с концепцией DAG (Directed Acyclic Graph) в Airflow и изучили как с помощью операторов создавать задачи. Мы также установили, что существует разнообразие видов операторов, которые позволяют нам эффективно писать код. Теперь давайте подробнее рассмотрим аргументы которые принимает DAG и operator.

**Аргументы DAG**

Начнем с кода который отвечает за создание нашего DAG:

```python
dag = DAG(dag_id='dag',
         default_args={'owner': 'airflow'},
         schedule_interval='@daily',
         start_date=days_ago(1) )
```

**dag_id**   
Это уникальное имя, оно будет в отражаться в интерфейсе. Не должно повторяться в рамках различных дагов.

![](https://ucarecdn.com/884ecb45-a143-4c86-bb92-c1b8602c50a5/)

**default_args**   
Это набор параметров которые будут применены к каждому `Operator`, пример использования ниже, в данном случае, если задача упадет, мы попробуем перезапустить её 1 раз, выполнится для всех задач внутри DAG.

```ini
args = {'retries': 1}

dag = DAG(
    dag_id='my_dag',
    default_args=args, # Передача списка параметров
    schedule_interval=timedelta(days=1),
    start_date= datetime(2023, 1, 1))
```

**start_date**   
С какой даты мы бы хотели запустить наш пайплайн, например нам хочется чтобы он начал работать с `2023-01-01` как в задаче выше. Вы можете задать любую дату/время которая была ранее чем текущий день. Вместе с использованием контекста задачи, о котором мы поговорим позже это используется для обеспечения идемпотентности наших ETL процессов.

**schedule_interval**   
Через какие интервалы нужно запускать задачу. Например такой код `timedelta(days=1)` будет запускать нашу задачу каждый день. Можно использовать уже знакомое вам `cron` выражение или набор предопределенных макросов, как например `@daily`

**Интервалы запуска**

Немного подробнее про последние 2 пункта, вместе они нужны чтобы сформировать _список дат_ когда будет запущена наш DAG, это работает по такой формуле 

ExecutionDate = StartDate + ScheduleIntervalExecutionDate = StartDate + ScheduleInterval

Для примера, сегодняшняя дата равна `2023-01-05` для нашей задачи мы выставляем такие параметры: 

```ini
start_date='2023-01-01' 
schedule_interval=timedelta(days=1)
```

Тогда будет сформирован список из 4 дат, после чего задача будет запущена за каждую дату: 

```yaml
2023-01-01
2023-01-02 
2023-01-03
2023-01-04
```

В нашем примере DAG запустится сразу, последовательно друг за другом, и отработает за все даты которые **уже** доступны, то есть весь список до сегодняшнего дня. Поэтому если вы выполняете запуск за прошлые даты, они отработают сразу, пока что вам может это показаться странным, но многое встанет на место когда мы изучим что такое _контекст задач_.  
  
Если не нужно запускать задачу за более ранние промежутки, можно указать `start_date=days_ago(1)` тогда запуск будет происходить только за последнюю дату (если вы запускаете по дням, если по часам то нужно указывать часы)

![](https://ucarecdn.com/aa930eef-4482-4684-aae6-a18851770bac/)  
  
Помимо этого есть ряд необязательных параметров, например:

- **retries** повторять N раз, если упал (применяется к **default_args** будет работать для каждого оператора отдельно)
- **on_failure_callback** вызовет функцию если DAG упадет (используется при информировании, про это будет позже)
- **trigger_rule** как нам реагировать на падение или успех наших задачек (про это будет чуть позже)
- **pool** выбирает пул куда будет слать свои таски, нужны чтобы ограничить параллельность (про это будет чуть позже)

Полный список аргументов можно посмотреть в документации, но поверьте он действительно большой, и позволяет гибко  настроить поведение вашего DAG в различных ситуациях.

**Аргументы Operator**

Изучим на примере PythonOperator. В случае с нашим пайплайном из первого шага, каждый оператор вызывает свою функцию со своими аргументами. Давайте рассмотрим пример с одним оператором и выясним, для чего предназначены передаваемые аргументы.

```python
load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
    op_kwargs={
        'tmp_file': '/tmp/file.csv',
        'table_name': 'table'
    }
)
```

**task_id**   
Также как и в случае с DAG, это имя нашего Task. В рамках 1 DAG должен быть уникальным.

**python_callable**  
Вызываемая функция, должна быть определена до вызова. Это Python функция в которой мы реализуем наш ETL. Стоит заметить что у каждого отдельного Оператора, такие функции разные, например `BashOperator` принимает на вход `bash` команду, Операторы доступа к базам данных например, `PostgresOperator` принимают параметр `sql` для sql кода.

**op_kwargs**   
Передаваемые аргументы в исполняемую функцию. Дело в том что передать аргументы напрямую нельзя, поэтому есть отдельный параметр для этого. Это аналогично такому коду. 

```go
op_kwargs = {'a':1, 'b': 2}

def func (a, b):
   print(a, b)

Ответ 1, 2
```

#### Дополнительные материалы

- [Список доступных аргументов для Airflow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)