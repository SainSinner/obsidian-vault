предыдущая [[3_2 Directed Acyclic Graph (DAG) в AirFlow]]
следующая [[3_2 Аргументы для DAG и Operator]]
документация по оператором официальная https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html

После того как мы с вами создали `DAG` пришло время наполнить его задачами. Здесь и далее нужно сделать важную пометку, что в Airflow понятие задачи состоит из двух сущностей, это `Task` и `Operator` которые на самом деле являются одним и тем же, но один создается из другого в момент исполнения задачи.

Начнем с понятия `Operator` Это то же `python Class`  аналогично DAG, но цель которого как бы описать задачу которая будет исполняться. Мы используем их для того чтобы задать что будет происходить в конкретно данной точке нашего `DAG` Существуют различные операторы, давайте посмотрим что они умеют делать:

**DummyOperator**

Начнем с самого простого он просто ничего не делает. Отрабатывает и передает управление дальше.

```python
from airflow import DAG
from datetime import timedelta
from airflow.utils.dates import days_ago
from airflow.operators.dummy_operator import DummyOperator
 
# Создадим объект класса DAG
dag =  DAG('dag', schedule_interval=timedelta(days=1), start_date=days_ago(1))

# Создадим dummy(пустые)команду
t1 = DummyOperator(task_id='task_1', dag=dag)
```

Результатом данного кода будет одна задача которая ничего не делает

![](https://ucarecdn.com/8b11a1a0-cb3f-4fd5-8887-6a9dce057fa6/)

На данном примере рассмотрим что такое `Task` Написав данный оператор мы с вами создали узел в нашем графе. Именно то для чего и был создан данная сущность, однако в момент исполнения `Operator` как бы превратился в `Task` Почему так происходит...

> Забегая немного вперед, когда мы создаем DAG мы назначаем ему аргументы в виде дата/время запуска (предыдущие шаги - `schedule_interval=timedelta(days=1), start_date=days_ago(1)`), которые по факту являются временными точками запуска задачи, каждый раз задавай начальную дату `start_date` и интервалы запуска `schedule_interval` будет создан массив дата/время пример `[2021-01-01, 2021-01-01 ....]` в которые будет запущен наш пайплайн. И в момент времени запуска Оператора, будет создан отдельный объект `Task` который и начнет исполнение. Мы разберемся с этим еще раз чуть позже.

Чтобы увидеть о чем идет речь, нажмите снова на зеленый квадратик и выберите Instance Details

![](https://ucarecdn.com/6b88268b-6153-48f4-ad7a-dd7c80e18bb6/)

После чего вам откроется набор параметров которые были созданы в момент начала выполнения задачи, включая время выполнения и другие параметры. То есть это уже будет `Task` или иначе  `Task Instance`. Подробнее про это мы поговорим на шаге на котором разберем что такое контекст задачи.

![](https://ucarecdn.com/8f93e086-fcfb-435f-a7b6-3d572d5f8200/)

Все что описано выше работает для всех операторов без исключения. Далее рассмотрим более полезные операторы:

**BashOperator**

```python
from airflow import DAG
from datetime import timedelta
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator

# Создадим объект класса DAG
dag =  DAG('dag', schedule_interval=timedelta(days=1), start_date=days_ago(1))

# Создадим несколько шагов, которые будут параллельно исполнять dummy(пустые)команды
t1 = BashOperator(task_id='task_1',
                  bash_command='cat /root/airflow/dags/dag.py',
                  dag=dag)
```

Данный оператор как очевидно исполнит Bash команду, поддерживается запуск из `.sh` файлов. Если посмотрите в лог файл то увидите что данная команда выводит текст данного скрипта. 

![](https://ucarecdn.com/16506bf3-8580-44ea-b6a8-f7f8ba3e6f65/)

Небольшое отступление, если вы хотите отладить ваш код и вам необходимо вручную запустить DAG, то справа сверху есть специальная кнопка которая запускает задачу в текущий момент времени. 

![](https://ucarecdn.com/4590fd7f-6e4c-4843-896f-87b1a31767d9/)

**PythonOperator**

Самый часто используемый на практике, он служит для запуска любого _python_ кода который вы напишите в вашей функции. Используется интерпретатор по умолчанию (однако есть специальный оператор _PythonVirtualenvOperator_, который создает локальное окружение для конкретной задачи и запускает код там)

Данный код запустит функцию `print_context`

```python
from airflow import DAG
from datetime import timedelta
from airflow.utils.dates import days_ago
from airflow.operators.python_operator import PythonOperator

# Создадим объект класса DAG
dag =  DAG('dag', schedule_interval=timedelta(days=1), start_date=days_ago(1))

def print_context():
    return 'Hello World'

run_this = PythonOperator(
    task_id='print_the_context',
    python_callable=print_context,
    dag=dag,
)
```

Данный код выведет текст `Hello World` в уже известный нам лог файл

![](https://ucarecdn.com/4a3a12a5-b8e7-4813-a51f-3f176cd976c1/)

**Другие операторы**

Все перечисленные выше операторы называются атомарными, потому что выполняют конкретное действие и на этом заканчивают исполнение. Например:

- Отправка электронной почты
- Выполнение команды в командной строке (bash)
- Выполнение кода на языке Python

Однако есть и другие виды операторов:

Трансферы - это тип операторов, который отвечает за перемещение данных из одного места в другое. Например, оператор `MySQLToGCSOperator` выполняет передачу данных из MySQL в Google Cloud Storage (GCS).

Сенсоры - это особый тип операторов, задача которых заключается в реагировании на определенное событие и передаче управления дальше. Для этой цели они используют специальный аргумент под названием `poke`, который возвращает либо True, либо False. Например, оператор `FileSensor` просто проверяет наличие файла на диске: если файл найден, он возвращает True; если файл отсутствует, оператор продолжает ожидать до истечения установленного времени ожидания (таймаута). 

С данными видами операторов мы познакомимся сильно позже в курсе, пока что просто знайте что они существуют.

**Заключение**

Важно понимать, что внутри оператора находится обычный код на языке Python, который можно написать самостоятельно (в конце курса мы этим займемся). 

**Пример в облаке**

Ссылка на DAG: [http://158.160.116.58:8001/tree?dag_id=0_Examples_3.2.3](http://158.160.116.58:8001/tree?dag_id=0_Examples_3.2.3) 

В данном DAG-е посмотрите как выводится в лог файл строка.

#### Дополнительные материалы

- [Список встроенных операторов](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html?highlight=operators#module-airflow.operators)