предыдущая [[3_4 Контекст и идемпотентность]]
следующая [[3_4 Контекст и идемпотентность]]


На прошло шаге [[3_2 Проектная работа перепишем ETL с применением Airflow]] мы написали простой пайплайн и обернули его в Airflow. Теперь давайте расширим его возможности с помощью макросов. Во всех задачах вам необходимо использовать макрос `{{ ds }}`  чтобы данные выгружались за определенный день. Для DAG вам нужно указать даты начала и конца исполнения задач с `2021-01-01` по `2021-01-04`. То есть скрипт отработает за каждый из 4 дней, и учтет контекст исполнения. [Ссылка](https://github.com/dm-novikov/stepik_airflow_course/tree/main/data_new) на файлы. [Курсы валют](https://github.com/datanlnja/airflow_course/tree/main/excangerate).

- Скачайте валюту за `{{ ds }}` и положите в CSV файл на диске (использовать PythonOperator чтобы скачать данные, можно использовать _pandas_)
- Скачайте логи финансовых транзакций за `{{ ds }}` и положите в CSV файл на диске (использовать PythonOperator чтобы скачать данные, можно использовать _pandas_)
- Объединить данные по дате и записать в CSV файл, положить файл нужно по следующему пути
    
    ```swift
    /usr/local/airflow/dags/sandbox/<ВАШ ID>/
    ```
    

![](https://ucarecdn.com/27f7316a-419c-4baf-affb-3ac7e5e4fac5/)

**Формат ответа:**  

Код который выполняет логику описанную выше

#### Ответ
```python
import pandas as pd
import sqlite3
import os
from datetime import datetime

from send_email import _send_email
from log_pas_email import username, password

CON = sqlite3.connect("example.db")

# Файлы записываются на диск, а не передаются в потоке исполнения
# это особенность работы Airflow которую мы обсудим позднее
# **context это необязательный аргумент, но мы будем его использовать далее
# это мощный инструмент который позволит нам писать идемпотентные скрипты

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.email import EmailOperator
from airflow.operators.python import PythonOperator

HOST = 'smtp.gmail.com'
TO = 'airflo2000@gmail.com'
FROM = 'g.cvyat@gmail.com'
URL_excangeRate = 'https://raw.githubusercontent.com/datanlnja/airflow_course/main/excangerate/'
URL_data = 'https://raw.githubusercontent.com/datanlnja/airflow_course/main/data/'


# FILE_PATH = 'excangeRateAndData.csv'
# DATE = "2021-01-01"


# Выгрузка данных с сайта
def extract_data(url, date, tmp_file, **context):
    url = url + date + '.csv'
    pd.read_csv(url).to_csv(tmp_file)


# Группировка данных
def merge_data(currency_file, tmp_file, tmp_agg_file, left_on="currency", right_on="currency_from", **context):
    data_currency = pd.read_csv(currency_file)
    data_currency["currency_from"] = data_currency["currency_from"].str.upper()
    data_currency["currency_to"] = data_currency["currency_to"].str.upper()
    data = pd.read_csv(tmp_file)
    merged_df = pd.merge(data, data_currency, left_on=left_on, right_on=right_on, how='left')
    merged_df = merged_df[['date_x', 'currency_from', 'currency_to', 'amount', 'value']]
    merged_df = merged_df.rename(columns={'currency_from': 'code', 'currency_to': 'base', 'date_x': 'date'})
    merged_df.to_csv(tmp_agg_file)


# Загрузка в базу данных
# Для тех кто не работал с pandas+sqlite
# data_frame.to_sql(...) автоматически создаст sqlite базу данных и таблицу
def load_data(table_name, tmp_file, conn=CON, **context):
    data = pd.read_csv(tmp_file)
    data["insert_time"] = pd.to_datetime("now")
    data.to_sql(table_name, conn, if_exists='replace', index=False)


# Отправка данных на почту
def send_email(tmp_file, username, password, host, port, to, From) -> None:
    data = pd.read_csv(tmp_file)
    """ Send to email
    """
    _send_email(data=data, username=username, password=password, host=host, port=port, to=to, From=From)


# Создаем DAG(контейнер) в который поместим наши задачи
# Для DAG-а характерны нужно задать следующие обязательные атрибуты
# - Уникальное имя
# - Интервал запусков
# - Начальная точка запуска

dag = DAG(dag_id='AirFlow_3_4_final',  # Имя нашего дага, уникальное
          default_args={'owner': 'airflow'},  # Список необязательных аргументов
          schedule_interval='@daily',  # Интервал запусков, в данном случае 1 раз в день 24:00
          start_date=datetime(2021, 1, 1),
          end_date=datetime(2021, 1, 4),
          max_active_runs=1, # позволяет ограничить количество запущенных дагов одновременно
          )

# Создадим задачи, которые будут запускать функции в питоне
# Выгрузка
extract_data_main = PythonOperator(
    task_id='extract_data_main',  # Имя задачи внутри Dag
    python_callable=extract_data,  # Запускаемая Python функция, описана выше

    # Чтобы передать аргументы в нашу функцию
    # их следует передавать через следующий код
    op_kwargs={
        'url': f'{URL_data}',
        'date': '{{ ds }}',
        'tmp_file': '/tmp/airFlow/file.csv'},
    dag=dag
)

# Выгрузка курса валют
extract_data_currency = PythonOperator(
    task_id='extract_data_currency',  # Имя задачи внутри Dag
    python_callable=extract_data,  # Запускаемая Python функция, описана выше

    # Чтобы передать аргументы в нашу функцию
    # их следует передавать через следующий код
    op_kwargs={
        'url': f'{URL_excangeRate}',
        'date': '{{ ds }}',
        'tmp_file': '/tmp/airFlow/file_currency.csv'},
    dag=dag
)

# Трансформация данных
merge_data = PythonOperator(
    task_id='merge_data',  # Имя задачи внутри Dag
    python_callable=merge_data,  # Запускаемая Python функция, описана выше
    # Чтобы передать аргументы в нашу функцию
    # их следует передавать через следующий код
    op_kwargs={
        'tmp_file': '/tmp/airFlow/file.csv',
        'tmp_agg_file': '/tmp/airFlow/file_agg.csv',
        'currency_file': '/tmp/airFlow/file_currency.csv'},
    dag=dag
)

# Загрузка в таблицу
load_data = PythonOperator(
    task_id='load_data',  # Имя задачи внутри Dag
    python_callable=load_data,  # Запускаемая Python функция, описана выше
    # Чтобы передать аргументы в нашу функцию
    # их следует передавать через следующий код
    op_kwargs={
        'tmp_file': '/tmp/airFlow/file_agg.csv',
        'table_name': 'test_table', },
    dag=dag
)

# Отправка Email
send_email = PythonOperator(
    task_id='send_email',  # Имя задачи внутри Dag
    python_callable=send_email,  # Запускаемая Python функция, описана выше
    # Чтобы передать аргументы в нашу функцию
    # их следует передавать через следующий код
    op_kwargs={
        'tmp_file': '/tmp/airFlow/file_agg.csv',
        'username': f'{username}',
        'password': f'{password}',
        'host': f'{HOST}',
        'port': 587,
        'to': f'{TO}',
        'From': f'{FROM}',
        'table_name': 'test_table', },
    dag=dag
)

# Создадим порядок выполнения задач
extract_data_main >> extract_data_currency >> merge_data >> [load_data, send_email]

# Документация
extract_data_main.doc_md = "Извлекает данные о продажах"
extract_data_currency.doc_md = "Извлекает данные о курсе валют"
merge_data.doc_md = "Мерджит данные продаж и курса валют"
load_data.doc_md = "Загружает данные в базу данных"
send_email.doc_md = "Отправляет данные на почту"
dag.doc_md = "Извлекает, трансформирует, загружает в БД, отправляет на почту"
```