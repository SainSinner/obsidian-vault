Мы уже обсуждали, что в Airflow логи хранятся и доступны для просмотра. Фактически, Airflow сохраняет множество логов, и иногда возникает необходимость в их управлении. Логи просто хранятся в файлах, размещенных в определенной директории. Для каждого запуска задачи создается отдельный файл, а также создаются логи для шедулера и веб-сервера.

Для сенсоров создается один файл на каждый запуск, и затем в него добавляются записи. Это позволяет избежать излишнего загромождения пользовательского интерфейса лишними логами. Мы рассмотрим этот момент более подробно позднее.

Существуют два способа хранения логов:

1. Локальное хранение (не рекомендуется для продакшн-решений): Логи сохраняются на локальной файловой системе, доступной на сервере, где работает Airflow. Этот подход не рекомендуется для продакшн-среды, так как логи могут занимать много места и не обеспечивать удобного управления и масштабируемости.
    
2. Хранение в облачных файловых системах (например, S3): Логи сохраняются в облачных файловых системах, таких как Amazon S3. Этот подход предпочтителен для продакшн-среды, так как облачные файловые системы предлагают масштабируемость, надежность и удобные инструменты для управления логами. Несколько ссылок для самостоятельного изучения: [Документация](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.html), [Видео настройка](https://www.youtube.com/watch?v=DKsWEmoqwZY)
    

Для локального хранения есть параметр в файле конфигурации

```ini
base_log_folder = /opt/airflow/logs
```

Для облачных решений есть набор параметров, вот несколько из них

```bash
remote_logging = False
remote_log_conn_id =
remote_base_log_folder =
...
```

Для логов есть также набор параметров которые улучшают отображение, например

```ini
colored_console_log = True
logging_level = INFO
```

**Пример в облаке** 

На сервере я реализовал данную функцию, вы можете просмотреть как это можно сделать если будете следовать моим шагам.

Первое, нужно развернуть Minio (это аналог S3 но на своем сервере). Для этого у меня существует отдельный сервис (на последнем модуле будет ссылка на данную сборку). На текущий момент просто примите это как данность.

Вот [ссылка](http://158.160.116.58:9006/) и доступы:

User: `student`  
Pass: `student`

Второй шаг, создание подключения, вам нужно найти подключение `my_s3_conn` в списке подключений

```ruby
http://158.160.116.58:8001/connection/list/
```

![](https://ucarecdn.com/b6912358-039f-4484-9e4e-0d7bd623ecc4/)

Специальные ключи доступа генерируются в интерфейсе Minio. На панели слева. 

Последний шаг, нужно изменить конфиг файл, в последнем модуле будет ссылка на готовую сборку где будет указаны доступы, сейчас я их просто продемонстрирую.

```ini
REMOTE_LOGGING=True
REMOTE_BASE_LOG_FOLDER=s3://airflow/logs
REMOTE_LOG_CONN_ID=my_s3_conn
ENCRYPT_S3_LOGS=False
```

Если вы все сделаете правильно то логи во всех DAG-s будут выглядеть вот так

![](https://ucarecdn.com/55fc33ea-ef8f-4540-840f-a54b6a9ad9ac/)