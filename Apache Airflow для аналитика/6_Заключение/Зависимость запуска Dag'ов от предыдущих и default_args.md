5https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#module-airflow.models.dag

Изначально задание выполнение Dag'ов происходит параллельно
Т.е. если мы создали Dag с параметрами

```python
with DAG('AirFlow_4_2_exchange_rates',  
         schedule_interval='@daily',  
         start_date=datetime(2024, 1, 1),  
         end_date=datetime(2024, 1, 4),  
         ) as dag:  
    # Создадим оператор для исполнения python функции  
    t1 = PythonOperator(  
        task_id='load_data',  
        python_callable=load_data,  
        op_kwargs={  
            'ds': '{{ds}}',  
            'tmp_file': '/tmp/airFlow/AirFlow_4_2_exchange_rates_',  
            'tmp_file_number': '0',  
            'tmp_file_type': '.csv'  
        }  
    )
```
Dag по каждой дате из прошлого 2024, 1, 1, 2024, 1, 2, 2024, 1, 3, 2024, 1, 4 запустится параллельно не дожидаясь успешного выполнения Dag'а за предыдущую дату.
Этого не хочется, если мы прописываем логику по формированию файла следующего вида
```python
# Если файла не существует то создаем его с заголовками, если существует то просто добавляем в него свежие данные  
if not os.path.exists(file_path):  
    df.to_csv(file_path, mode='w', header=True, index=True)  
else:  
    df.to_csv(file_path, mode='a', header=False, index=True)
```
в результате у нас получится ,что для dag'а 2024, 1, 1 и 2024, 1, 2 файла не существует и мы перезатрем в даге от 2024, 1, 2 результат работы дага 2024, 1, 1.
чтобы этого не происходило, необходимо добавить следующие параметры ==depends_on_past== и ==catchup==

```python
# Определяем аргументы по умолчанию для DAG  
default_args = {  
    'owner': 'airflow',  
    'depends_on_past': True,  # Устанавливаем зависимость от прошлого выполнения  
    'email_on_failure': False,  
    'email_on_retry': False,  
    'retries': 1,  
}  
  
with DAG('AirFlow_4_2_exchange_rates',  
         default_args=default_args,  
         schedule_interval='@daily',  
         start_date=datetime(2024, 1, 1),  
         end_date=datetime(2024, 1, 4),  
         catchup=True  # Учитываем все промежутки времени в периоде запуска  
         ) as dag:  
    # Создадим оператор для исполнения python функции  
    t1 = PythonOperator(  
        task_id='load_data',  
        python_callable=load_data,  
        op_kwargs={  
            'ds': '{{ds}}',  
            'tmp_file': '/tmp/airFlow/AirFlow_4_2_exchange_rates_',  
            'tmp_file_number': '0',  
            'tmp_file_type': '.csv'  
        }  
    )
```
более подробно можно прочитать здесь https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#module-airflow.models.dag

`default_args` в Airflow используется для задания общих параметров, которые применяются ко всем задачам в DAG. Это позволяет централизованно настроить поведение задач и упростить управление параметрами. Вот что означают различные параметры в `default_args`:

### Основные параметры `default_args`:

1. **`owner`**: Определяет владельца задач. Это может быть имя пользователя или роль, которая отвечает за выполнение задач. Это поле полезно для отслеживания и фильтрации задач по владельцу.

   ```python
   'owner': 'airflow',
   ```

2. **`depends_on_past`**: Устанавливает, должны ли задачи зависеть от успешного завершения предыдущего выполнения задачи с тем же `task_id` в предыдущем интервале времени. Если `True`, то текущая задача будет ждать завершения предыдущей задачи перед началом выполнения.

   ```python
   'depends_on_past': True,
   ```

3. **`email_on_failure`**: Определяет, должны ли отправляться уведомления по электронной почте, если задача завершается с ошибкой. По умолчанию `False`. Для отправки уведомлений нужно также настроить `email` и `email_on_retry`.

   ```python
   'email_on_failure': False,
   ```

4. **`email_on_retry`**: Определяет, должны ли отправляться уведомления по электронной почте при повторных попытках выполнения задачи. По умолчанию `False`.

   ```python
   'email_on_retry': False,
   ```

5. **`retries`**: Определяет количество попыток повторного выполнения задачи в случае неудачи. Если задача завершится с ошибкой, Airflow будет пытаться выполнить ее снова до достижения указанного количества попыток.

   ```python
   'retries': 1,
   ```

6. **`retry_delay`**: Устанавливает задержку между попытками повторного выполнения задачи. Это должно быть объект `timedelta`, который указывает, сколько времени ждать между попытками.

   ```python
   'retry_delay': timedelta(minutes=5),
   ```

7. **`start_date`**: Устанавливает дату и время, с которых DAG или задачи начинают выполняться. Если задача или DAG имеют заданный интервал времени, он будет использоваться для определения начала выполнения.

   ```python
   'start_date': datetime(2024, 1, 1),
   ```

8. **`end_date`**: Устанавливает дату и время, до которых DAG или задачи должны выполняться. После этого времени задачи не будут запускаться.

   ```python
   'end_date': datetime(2024, 1, 4),
   ```

9. **`schedule_interval`**: Определяет интервал времени, на основе которого DAG будет планироваться. Например, `@daily` означает, что DAG будет выполняться ежедневно.

   ```python
   'schedule_interval': '@daily',
   ```

10. **`queue`**: Определяет очередь, в которую будет отправлена задача для выполнения. Это может быть полезно, если у вас есть несколько очередей задач и вы хотите распределять задачи между ними.

    ```python
    'queue': 'default',
    ```

11. **`priority_weight`**: Устанавливает вес задачи, который может быть использован для определения приоритета выполнения задачи. Более высокий вес означает более высокий приоритет.

    ```python
    'priority_weight': 10,
    ```

12. **`pool`**: Определяет пул, в который будет отправлена задача. Пулы используются для ограничения одновременного выполнения задач определенного типа.

    ```python
    'pool': 'default_pool',
    ```

13. **`end_date`**: Устанавливает конечную дату и время, до которых DAG или задачи будут выполняться.

    ```python
    'end_date': datetime(2024, 1, 4),
    ```

### Пример использования:

```python
default_args = {
    'owner': 'airflow',
    'depends_on_past': True,  # Задачи зависят от успешного завершения предыдущих выполнений
    'email_on_failure': False,  # Не отправлять email при ошибках
    'email_on_retry': False,  # Не отправлять email при повторных попытках
    'retries': 1,  # Количество попыток повторного выполнения
    'retry_delay': timedelta(minutes=5),  # Задержка между попытками
    'start_date': datetime(2024, 1, 1),  # Дата начала выполнения
    'end_date': datetime(2024, 1, 4),  # Дата окончания выполнения
    'catchup': True  # Выполнение задач за пропущенные промежутки времени
}
```

Эти параметры можно использовать для тонкой настройки поведения ваших DAG и задач, чтобы соответствовать требованиям вашего рабочего процесса.