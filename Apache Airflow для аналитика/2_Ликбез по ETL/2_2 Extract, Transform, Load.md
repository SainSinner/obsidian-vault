предыдущая [[2_2 Почему нам нужен Airflow]]
следующая [[2_2 Directed Acyclic Graph (DAG)]]

**Extract, Transform, Load**

Extract, Transform, Load является процессом последовательной обработки данных. Он представляет собой последовательность шагов, которые позволяют извлекать данные из различных источников, преобразовывать их в нужный формат и загружать в целевую систему для анализа, отчетности и хранения. Давайте разберем наш предыдущий код этап за этапом:

1. Извлекаем данные (extract_data): Этот этап включает сбор данных из источника, который в действительности может быть любым.
    
2. Преобразуем данные (transform_data): Здесь осуществляются агрегация, обогащение и различные манипуляции с данными.
    
3. Загружаем данные в базу данных (load_data): Данные загружаются в какое-либо хранилище, к которому будет обращаться заказчик.
    
4. Отправляем результат на электронную почту (send_email).
    

ETL на самом деле представляет собой класс задач, которые могут иметь разные решения в зависимости от источников данных, хранилища, частоты обновлений и объема данных. Задачи ETL также могут различаться в зависимости от используемого технологического стека компании. Например:

- Перемещение данных из одной базы данных в другую.
- Сбор кликстрима с веб-сайта и сохранение в хранилище.
- Загрузка данных через API из внешних источников.
- Написание скриптов для ежедневных отчетов заказчикам.

Задачи ETL характерны для хранилищ данных (Data Warehouse, DWH). Data Warehouse - это тип аналитической базы данных, с которой работают аналитики. В таком хранилище лежат предварительно обработанные данные. Примерами таких баз данных могут быть ClickHouse, Redshift, Greenplum или даже Postgres.

![](https://ucarecdn.com/10102bfe-ad54-4a4e-b2ed-e86d2bcfcf58/)

Пример данных, которые могут лежать в таком хранилище

|   |   |   |   |   |
|---|---|---|---|---|
|id|text|value|country|url|
|123|Описание сущности|100|Россия|site.ru|

  
**Extract, Load, Transform**

Также существует подход ELT, который обычно используется в так называемых Data Lake. В этом случае шаги нашей последовательности просто меняются местами, но суть остается той же.

1. Выгружаем и загружаем сырые данные в Data Lake.
2. Затем выполняем преобразования для перевода их в структурированный формат.

Data Lake - это, как правило, распределенное файловое хранилище, такое как HDFS, S3, GFS. Здесь мы сохраняем данные в наиболее первоначальном виде (сыром). Если у нас есть URL-ссылка, мы сохраняем ее в исходном состоянии и затем выполняем необходимые преобразования уже на этапе работы с данными.

![](https://ucarecdn.com/3666762c-d9bc-4c3c-a5fe-72af49d300b4/)

Пример данных которые могут лежать в таком хранилище

|   |   |   |
|---|---|---|
|id|event_time|json|
|123|2021-01-01 01:04:59|{"url": "http://site.ru", "html": "<html>....</html>"}|
**Полезные материалы**

- [Отличные видеоуроки о том что такое ETL/ELT](https://www.youtube.com/watch?v=3IRU-E_BnYc&list=PLkcP_moW_BpNgbIdIeiUDR01vxmuSpY-L&index=2)
- [Обзор того что такое DWH и Data Lake](https://www.youtube.com/watch?v=pRiRpOg64-4)